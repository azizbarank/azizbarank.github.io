<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Simple Guide to Building and Deploying Transformer Models with Hugging Face | Aziz Baran</title><meta name=keywords content><meta name=description content="In this post, I want to talk about how to use Hugging Face to build and deploy our NLP model as a web application. To do this, I’m going to use the Distilled Turkish BERT model and fine-tune it on a Turkish review dataset. Later, I’m going to make use of the Streamlit library and Hugging Face Spaces to showcase the final model so that the other people can use it without any code."><meta name=author content="Aziz Baran"><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3299c596a7007118365635c056dd427dace22b7b8c1341fdef6fa6c31359ba10.css integrity="sha256-MpnFlqcAcRg2VjXAVt1CfaziK3uME0H972+mwxNZuhA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Simple Guide to Building and Deploying Transformer Models with Hugging Face"><meta property="og:description" content="In this post, I want to talk about how to use Hugging Face to build and deploy our NLP model as a web application. To do this, I’m going to use the Distilled Turkish BERT model and fine-tune it on a Turkish review dataset. Later, I’m going to make use of the Streamlit library and Hugging Face Spaces to showcase the final model so that the other people can use it without any code."><meta property="og:type" content="article"><meta property="og:url" content="https://azizbarank.github.io/post/deployment/"><meta property="og:image" content="https://azizbarank.github.io/%3Cimage%20path/url%3E"><meta property="article:section" content="post"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://azizbarank.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="Simple Guide to Building and Deploying Transformer Models with Hugging Face"><meta name=twitter:description content="In this post, I want to talk about how to use Hugging Face to build and deploy our NLP model as a web application. To do this, I’m going to use the Distilled Turkish BERT model and fine-tune it on a Turkish review dataset. Later, I’m going to make use of the Streamlit library and Hugging Face Spaces to showcase the final model so that the other people can use it without any code."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://azizbarank.github.io/post/"},{"@type":"ListItem","position":2,"name":"Simple Guide to Building and Deploying Transformer Models with Hugging Face","item":"https://azizbarank.github.io/post/deployment/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Simple Guide to Building and Deploying Transformer Models with Hugging Face","name":"Simple Guide to Building and Deploying Transformer Models with Hugging Face","description":"In this post, I want to talk about how to use Hugging Face to build and deploy our NLP model as a web application. To do this, I’m going to use the Distilled Turkish BERT model and fine-tune it on a Turkish review dataset. Later, I’m going to make use of the Streamlit library and Hugging Face Spaces to showcase the final model so that the other people can use it without any code.","keywords":[],"articleBody":"In this post, I want to talk about how to use Hugging Face to build and deploy our NLP model as a web application. To do this, I’m going to use the Distilled Turkish BERT model and fine-tune it on a Turkish review dataset. Later, I’m going to make use of the Streamlit library and Hugging Face Spaces to showcase the final model so that the other people can use it without any code.\nPreliminary Step Before beginning the whole process, let’s install the necessary packages and import the libraries:\n#huggingface_hub to login into our huggingface account later !pip install datasets transformers huggingface_hub import torch torch.cuda.is_available() !sudo apt-get install git-lfs 1. Loading our dataset via the “Datasets” library of Hugging Face from datasets import load_dataset review = load_dataset(\"sepidmnorozy/Turkish_sentiment\") 2. Using Hugging Face’s “Transformers” library for preprocessing and loading our model To briefly explain, when our dataset is ready to be preprocessed, we need to use the “Transformers” library both to preprocess it, and load the model itself for the fine-tuning with it. Therefore, this whole process can be divided into two subsections: a)\tLoading our tokenizer using the “AutoTokenizer” class of the library\nIn the context of the transformers, tokenizers are used to do the preprocessing to the raw data we have. Therefore, before any fine-tuning, we need to load the tokenizer of our current model via the AutoTokenizer class:\nfrom transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/distilbert-base-turkish-cased\") In this case, since we are dealing with a relatively large dataset, we can use some of its examples to avoid any memory storage/lack of any computational resource. Let’s take random 2000 and 200 examples of the training and test sets, respectively.\nsmall_train_dataset = review[\"train\"].shuffle(seed=42).select([i for i in list(range(2000))]) small_test_dataset = review[\"test\"].shuffle(seed=42).select([i for i in list(range(200))]) Now, since we have a dataset rather than a single string to preprocess, we need to define a function:\ndef preprocess_function(examples): return tokenizer(examples[\"text\"], truncation=True) Additionally, to apply such a function, we need to use the map() method:\ntokenized_train = small_train_dataset.map(preprocess_function, batched=True) tokenized_test = small_test_dataset.map(preprocess_function, batched=True) b)\tLoading our model itself\nNow that we have made our data ready for the fine-tuning, we can load our very model:\nfrom transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(\"dbmdz/distilbert-base-turkish-cased\", num_labels=2) Since we want to use our model for binary classification, we set the num_labels to 2.\n3. Defining Metrics For a complete fine-tuning and evaluation, we need to define our metric (which is “accuracy” in this case) beforehand. To do that, we need to use NumPy and the Datasets library:\nimport numpy as np from datasets import load_metric def compute_metrics(eval_pred): load_accuracy = load_metric(\"accuracy\") load_f1 = load_metric(\"f1\") logits, labels = eval_pred predictions = np.argmax(logits, axis=-1) accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"] f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"] return {\"accuracy\": accuracy, \"f1\": f1} 4. Loging into our Hugging Face Account Before doing fine-tuning, to be able to push our resulted model to the hub, we need to login our credentials:\nfrom huggingface_hub import notebook_login notebook_login() 5. Fine-tuning our Model Through its Transformers library, Hugging Face provides us with a convenient Trainer API to fine-tune the models with a few line of codes. Basically, to use it, we need to provide it with arguments to be taken into account during fine-tuning. Therefore, initially, our step is to define these very training arguments. Let’s load the TrainingArguments and Trainer first:\nfrom transformers import TrainingArguments, Trainer Now, let’s define the arguments and then use them in trainer:\nrepo_name = \"distilbert-based-turkish-cased-sentiment\" training_args = TrainingArguments( output_dir='./results', learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=2, weight_decay=0.01, save_strategy=\"epoch\", push_to_hub=True, ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test, tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics, ) Now, we are ready to train our model:\ntrainer.train() # TrainOutput(global_step=250, training_loss=0.577752197265625, metrics={'train_runtime': #49.4941, 'train_samples_per_second': 80.818, 'train_steps_per_second': 5.051, 'total_flos': #119551827287040.0, 'train_loss': 0.577752197265625, 'epoch': 2.0}) 6. Evaluating and pushing our model to the hub: Now that we have fine-tuned our model, we can use our pre-defined metrics to evaluate it on our test set, and then finally push it to the hub:\ntrainer.evaluate() # {'eval_loss': 0.5146501064300537, #'eval_accuracy': 0.78, #'eval_f1': 0.83206106870229, #'eval_runtime': 1.7692, #'eval_samples_per_second': 113.047, #'eval_steps_per_second': 7.348, #'epoch': 2.0} trainer.push_to_hub() 7. Using Streamlit to create an app: The first for the deployment our model as a web app is an actual Python code that will “convert” our pure model to an app. To do this, we can use the Streamlit library. Here is a full example of how to use it to create an app script:\n#installing and importing the necessary packages import os os.system(\"pip install torch\") os.system(\"pip install transformers\") os.system(\"pip install sentencepiece\") import streamlit as st from transformers import pipeline #Loading our model from transformers import AutoTokenizer, AutoModelForSequenceClassification tokenizer = AutoTokenizer.from_pretrained(\"azizbarank/distilbert-base-turkish-cased-sentiment\") model = AutoModelForSequenceClassification.from_pretrained(\"azizbarank/distilbert-base-turkish-cased-sentiment\") #Defining a function to classify the input text def classify(text): cls= pipeline(\"text-classification\",model=model, tokenizer=tokenizer) return cls(text)[0]['label'] # Making the layout of our web application through the st.container() function site_header = st.container() text_input = st.container() model_results = st.container() # Writing some content in these containers with site_header: st.title('Sentiment Analysis') st.markdown( \"\"\" This is my web application! \"\"\" ) with text_input: st.header('Is Your Review Considered Positive or Negative?') st.write(\"\"\"*Please note that predictions are based on how the model was trained, so it may not be an accurate representation.*\"\"\") user_text = st.text_input('Enter Text', max_chars=300) with model_results: st.subheader('Prediction:') if user_text: prediction = classify(user_text) if prediction == \"LABEL_0\": st.subheader('**Negative**') else: st.subheader('**Positive**') st.text('') 8. Using Hugging Face Spaces for the final deployment: After this, you can simply navigate to your Hugging Face profile and click “New Space”. Then, by choosing the “Add File” option, we can simply copy and paste our script we just created using Python and Streamlit. The important thing is that since our code is a Python script, we should define it in the name of the file section above by simply adding the “.py” extension to the name of the file we make. After these, within a few seconds, the app should start running smoothly to be used by other people. I include here an example from my personal Hugging Face profile:\nA Personal Example: Conclusion: In this post, we fine-tuned our language model on a dataset using Hugging Face’s Transformers and Datasets library. Then, we made use of the Streamlit library with Hugging Face Spaces to deploy our resulted language model as an interactive web app.\nThanks for reading and I hope this post will be of help to anyone wishing to build and deploy various language models.\n","wordCount":"1051","inLanguage":"en","image":"https://azizbarank.github.io/%3Cimage%20path/url%3E","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Aziz Baran"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://azizbarank.github.io/post/deployment/"},"publisher":{"@type":"Organization","name":"Aziz Baran","logo":{"@type":"ImageObject","url":"https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://azizbarank.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://azizbarank.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://azizbarank.github.io/about/ title=About><span>About</span></a></li><li><a href=https://azizbarank.github.io/post/ title=Post><span>Post</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://azizbarank.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://azizbarank.github.io/post/>Posts</a></div><h1 class=post-title>Simple Guide to Building and Deploying Transformer Models with Hugging Face</h1><div class=post-meta>5 min&nbsp;·&nbsp;1051 words&nbsp;·&nbsp;Aziz Baran&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/post/deployment.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#preliminary-step>Preliminary Step</a></li><li><a href=#1-loading-our-dataset-via-the-datasets-library-of-hugging-face>1. Loading our dataset via the “Datasets” library of Hugging Face</a></li><li><a href=#2-using-hugging-faces-transformers-library-for-preprocessing-and-loading-our-model>2. Using Hugging Face’s “Transformers” library for preprocessing and loading our model</a></li><li><a href=#3-defining-metrics>3. Defining Metrics</a></li><li><a href=#4-loging-into-our-hugging-face-account>4. Loging into our Hugging Face Account</a></li><li><a href=#5-fine-tuning-our-model>5. Fine-tuning our Model</a></li><li><a href=#6-evaluating-and-pushing-our-model-to-the-hub>6. Evaluating and pushing our model to the hub:</a></li><li><a href=#7-using-streamlit-to-create-an-app>7. Using Streamlit to create an app:</a></li><li><a href=#8-using-hugging-face-spaces-for-the-final-deployment>8. Using Hugging Face Spaces for the final deployment:</a></li><li><a href=#a-personal-example>A Personal Example:</a></li><li><a href=#conclusion>Conclusion:</a></li></ul></nav></div></details></div><div class=post-content><p>In this post, I want to talk about how to use Hugging Face to build and deploy our NLP model as a web application.
To do this, I’m going to use the <a href=https://huggingface.co/dbmdz/distilbert-base-turkish-cased>Distilled Turkish BERT</a> model and fine-tune it on a Turkish review dataset. Later, I’m going to make use of the Streamlit library and Hugging Face Spaces to showcase the final model so that the other people can use it without any code.</p><h2 id=preliminary-step>Preliminary Step<a hidden class=anchor aria-hidden=true href=#preliminary-step>#</a></h2><p>Before beginning the whole process, let’s install the necessary packages and import the libraries:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#huggingface_hub to login into our huggingface account later</span>
</span></span><span class=line><span class=cl><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=n>datasets</span> <span class=n>transformers</span> <span class=n>huggingface_hub</span> 
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span><span class=n>sudo</span> <span class=n>apt</span><span class=o>-</span><span class=n>get</span> <span class=n>install</span> <span class=n>git</span><span class=o>-</span><span class=n>lfs</span>
</span></span></code></pre></div><h2 id=1-loading-our-dataset-via-the-datasets-library-of-hugging-face>1. Loading our dataset via the “Datasets” library of Hugging Face<a hidden class=anchor aria-hidden=true href=#1-loading-our-dataset-via-the-datasets-library-of-hugging-face>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span>
</span></span><span class=line><span class=cl><span class=n>review</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;sepidmnorozy/Turkish_sentiment&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=2-using-hugging-faces-transformers-library-for-preprocessing-and-loading-our-model>2. Using Hugging Face’s “Transformers” library for preprocessing and loading our model<a hidden class=anchor aria-hidden=true href=#2-using-hugging-faces-transformers-library-for-preprocessing-and-loading-our-model>#</a></h2><p>To briefly explain, when our dataset is ready to be preprocessed, we need to use the “Transformers” library both to preprocess it, and load the model itself for the fine-tuning with it. Therefore, this whole process can be divided into two subsections:
a) Loading our tokenizer using the “AutoTokenizer” class of the library</p><p>In the context of the transformers, tokenizers are used to do the preprocessing to the raw data we have. Therefore, before any fine-tuning, we need to load the tokenizer of our current model via the AutoTokenizer class:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;dbmdz/distilbert-base-turkish-cased&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>In this case, since we are dealing with a relatively large dataset, we can use some of its examples to avoid any memory storage/lack of any computational resource. Let’s take random 2000 and 200 examples of the training and test sets, respectively.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>small_train_dataset</span> <span class=o>=</span> <span class=n>review</span><span class=p>[</span><span class=s2>&#34;train&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span><span class=o>.</span><span class=n>select</span><span class=p>([</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>2000</span><span class=p>))])</span>
</span></span><span class=line><span class=cl><span class=n>small_test_dataset</span> <span class=o>=</span> <span class=n>review</span><span class=p>[</span><span class=s2>&#34;test&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span><span class=o>.</span><span class=n>select</span><span class=p>([</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>200</span><span class=p>))])</span>
</span></span></code></pre></div><p>Now, since we have a dataset rather than a single string to preprocess, we need to define a function:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>preprocess_function</span><span class=p>(</span><span class=n>examples</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>examples</span><span class=p>[</span><span class=s2>&#34;text&#34;</span><span class=p>],</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>Additionally, to apply such a function, we need to use the map() method:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tokenized_train</span> <span class=o>=</span> <span class=n>small_train_dataset</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>preprocess_function</span><span class=p>,</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenized_test</span> <span class=o>=</span> <span class=n>small_test_dataset</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>preprocess_function</span><span class=p>,</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>b) Loading our model itself</p><p>Now that we have made our data ready for the fine-tuning, we can load our very model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForSequenceClassification</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;dbmdz/distilbert-base-turkish-cased&#34;</span><span class=p>,</span> <span class=n>num_labels</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></div><p>Since we want to use our model for binary classification, we set the num_labels to 2.</p><h2 id=3-defining-metrics>3. Defining Metrics<a hidden class=anchor aria-hidden=true href=#3-defining-metrics>#</a></h2><p>For a complete fine-tuning and evaluation, we need to define our metric (which is “accuracy” in this case) beforehand. To do that, we need to use NumPy and the Datasets library:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_metric</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_metrics</span><span class=p>(</span><span class=n>eval_pred</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=n>load_accuracy</span> <span class=o>=</span> <span class=n>load_metric</span><span class=p>(</span><span class=s2>&#34;accuracy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>load_f1</span> <span class=o>=</span> <span class=n>load_metric</span><span class=p>(</span><span class=s2>&#34;f1&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=n>logits</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>eval_pred</span>
</span></span><span class=line><span class=cl>   <span class=n>predictions</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>accuracy</span> <span class=o>=</span> <span class=n>load_accuracy</span><span class=o>.</span><span class=n>compute</span><span class=p>(</span><span class=n>predictions</span><span class=o>=</span><span class=n>predictions</span><span class=p>,</span> <span class=n>references</span><span class=o>=</span><span class=n>labels</span><span class=p>)[</span><span class=s2>&#34;accuracy&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>   <span class=n>f1</span> <span class=o>=</span> <span class=n>load_f1</span><span class=o>.</span><span class=n>compute</span><span class=p>(</span><span class=n>predictions</span><span class=o>=</span><span class=n>predictions</span><span class=p>,</span> <span class=n>references</span><span class=o>=</span><span class=n>labels</span><span class=p>)[</span><span class=s2>&#34;f1&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;accuracy&#34;</span><span class=p>:</span> <span class=n>accuracy</span><span class=p>,</span> <span class=s2>&#34;f1&#34;</span><span class=p>:</span> <span class=n>f1</span><span class=p>}</span>
</span></span></code></pre></div><h2 id=4-loging-into-our-hugging-face-account>4. Loging into our Hugging Face Account<a hidden class=anchor aria-hidden=true href=#4-loging-into-our-hugging-face-account>#</a></h2><p>Before doing fine-tuning, to be able to push our resulted model to the hub, we need to login our credentials:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>huggingface_hub</span> <span class=kn>import</span> <span class=n>notebook_login</span>
</span></span><span class=line><span class=cl><span class=n>notebook_login</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=5-fine-tuning-our-model>5. Fine-tuning our Model<a hidden class=anchor aria-hidden=true href=#5-fine-tuning-our-model>#</a></h2><p>Through its Transformers library, Hugging Face provides us with a convenient Trainer API to fine-tune the models with a few line of codes. Basically, to use it, we need to provide it with arguments to be taken into account during fine-tuning. Therefore, initially, our step is to define these very training arguments.
Let’s load the TrainingArguments and Trainer first:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>TrainingArguments</span><span class=p>,</span> <span class=n>Trainer</span>
</span></span></code></pre></div><p>Now, let’s define the arguments and then use them in trainer:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>repo_name</span> <span class=o>=</span> <span class=s2>&#34;distilbert-based-turkish-cased-sentiment&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>training_args</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>output_dir</span><span class=o>=</span><span class=s1>&#39;./results&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>learning_rate</span><span class=o>=</span><span class=mf>2e-5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>per_device_eval_batch_size</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>num_train_epochs</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>save_strategy</span><span class=o>=</span><span class=s2>&#34;epoch&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>push_to_hub</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>args</span><span class=o>=</span><span class=n>training_args</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>train_dataset</span><span class=o>=</span><span class=n>tokenized_train</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>eval_dataset</span><span class=o>=</span><span class=n>tokenized_test</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>data_collator</span><span class=o>=</span><span class=n>data_collator</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>compute_metrics</span><span class=o>=</span><span class=n>compute_metrics</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Now, we are ready to train our model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># TrainOutput(global_step=250, training_loss=0.577752197265625, metrics={&#39;train_runtime&#39;: #49.4941, &#39;train_samples_per_second&#39;: 80.818, &#39;train_steps_per_second&#39;: 5.051, &#39;total_flos&#39;: #119551827287040.0, &#39;train_loss&#39;: 0.577752197265625, &#39;epoch&#39;: 2.0})</span>
</span></span></code></pre></div><h2 id=6-evaluating-and-pushing-our-model-to-the-hub>6. Evaluating and pushing our model to the hub:<a hidden class=anchor aria-hidden=true href=#6-evaluating-and-pushing-our-model-to-the-hub>#</a></h2><p>Now that we have fine-tuned our model, we can use our pre-defined metrics to evaluate it on our test set, and then finally push it to the hub:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>evaluate</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># {&#39;eval_loss&#39;: 0.5146501064300537,</span>
</span></span><span class=line><span class=cl><span class=c1>#&#39;eval_accuracy&#39;: 0.78,</span>
</span></span><span class=line><span class=cl><span class=c1>#&#39;eval_f1&#39;: 0.83206106870229,</span>
</span></span><span class=line><span class=cl><span class=c1>#&#39;eval_runtime&#39;: 1.7692,</span>
</span></span><span class=line><span class=cl><span class=c1>#&#39;eval_samples_per_second&#39;: 113.047,</span>
</span></span><span class=line><span class=cl><span class=c1>#&#39;eval_steps_per_second&#39;: 7.348,</span>
</span></span><span class=line><span class=cl><span class=c1>#&#39;epoch&#39;: 2.0}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>push_to_hub</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=7-using-streamlit-to-create-an-app>7. Using Streamlit to create an app:<a hidden class=anchor aria-hidden=true href=#7-using-streamlit-to-create-an-app>#</a></h2><p>The first for the deployment our model as a web app is an actual Python code that will “convert” our pure model to an app. To do this, we can use the Streamlit library. Here is a full example of how to use it to create an app script:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#installing and importing the necessary packages</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>system</span><span class=p>(</span><span class=s2>&#34;pip install torch&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>system</span><span class=p>(</span><span class=s2>&#34;pip install transformers&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>system</span><span class=p>(</span><span class=s2>&#34;pip install sentencepiece&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>streamlit</span> <span class=k>as</span> <span class=nn>st</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>pipeline</span>
</span></span><span class=line><span class=cl><span class=c1>#Loading our model</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForSequenceClassification</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;azizbarank/distilbert-base-turkish-cased-sentiment&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;azizbarank/distilbert-base-turkish-cased-sentiment&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#Defining a function to classify the input text</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>classify</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=bp>cls</span><span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&#34;text-classification&#34;</span><span class=p>,</span><span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>cls</span><span class=p>(</span><span class=n>text</span><span class=p>)[</span><span class=mi>0</span><span class=p>][</span><span class=s1>&#39;label&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># Making the layout of our web application through the st.container() function</span>
</span></span><span class=line><span class=cl><span class=n>site_header</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>container</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>text_input</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>container</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model_results</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>container</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Writing some content in these containers</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>site_header</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>st</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Sentiment Analysis&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>st</span><span class=o>.</span><span class=n>markdown</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    This is my web application!
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>text_input</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>st</span><span class=o>.</span><span class=n>header</span><span class=p>(</span><span class=s1>&#39;Is Your Review Considered Positive or Negative?&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>st</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s2>&#34;&#34;&#34;*Please note that predictions are based on how the model was trained, so it may not be an accurate representation.*&#34;&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>user_text</span> <span class=o>=</span> <span class=n>st</span><span class=o>.</span><span class=n>text_input</span><span class=p>(</span><span class=s1>&#39;Enter Text&#39;</span><span class=p>,</span> <span class=n>max_chars</span><span class=o>=</span><span class=mi>300</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>model_results</span><span class=p>:</span>    
</span></span><span class=line><span class=cl>    <span class=n>st</span><span class=o>.</span><span class=n>subheader</span><span class=p>(</span><span class=s1>&#39;Prediction:&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>user_text</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>prediction</span> <span class=o>=</span> <span class=n>classify</span><span class=p>(</span><span class=n>user_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>prediction</span> <span class=o>==</span> <span class=s2>&#34;LABEL_0&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>st</span><span class=o>.</span><span class=n>subheader</span><span class=p>(</span><span class=s1>&#39;**Negative**&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>st</span><span class=o>.</span><span class=n>subheader</span><span class=p>(</span><span class=s1>&#39;**Positive**&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>st</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=s1>&#39;&#39;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=8-using-hugging-face-spaces-for-the-final-deployment>8. Using Hugging Face Spaces for the final deployment:<a hidden class=anchor aria-hidden=true href=#8-using-hugging-face-spaces-for-the-final-deployment>#</a></h2><p>After this, you can simply navigate to your Hugging Face profile and click “New Space”. Then, by choosing the “Add File” option, we can simply copy and paste our script we just created using Python and Streamlit.
The important thing is that since our code is a Python script, we should define it in the name of the file section above by simply adding the “.py” extension to the name of the file we make.
After these, within a few seconds, the app should start running smoothly to be used by other people. I include here an example from my personal Hugging Face profile:</p><h2 id=a-personal-example>A Personal Example:<a hidden class=anchor aria-hidden=true href=#a-personal-example>#</a></h2><p><img loading=lazy src=https://raw.githubusercontent.com/azizbarank/Turkish-Sentiment-Analyser/main/web_app.png alt=Image></p><h2 id=conclusion>Conclusion:<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>In this post, we fine-tuned our language model on a dataset using Hugging Face’s Transformers and Datasets library. Then, we made use of the Streamlit library with Hugging Face Spaces to deploy our resulted language model as an interactive web app.</p><p>Thanks for reading and I hope this post will be of help to anyone wishing to build and deploy various language models.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://azizbarank.github.io/post/resources/><span class=title>« Prev</span><br><span>Dealing with Lack of Computational Resources in NLP</span></a>
<a class=next href=https://azizbarank.github.io/post/transformers/><span class=title>Next »</span><br><span>Transformers as Feature Extractors</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://azizbarank.github.io/>Aziz Baran</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>