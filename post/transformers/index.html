<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformers as Feature Extractors | Aziz Baran</title><meta name=keywords content><meta name=description content="Transformers continue to be one of the most frequently used models for various NLP tasks since 2017. However, due to their high computational resource requirements and difficult maintenance, they may not be the most efficient choice out there all the time. This is especially true for simple sentiment classification tasks. In such circumstances, among the alternatives, there is the feature-based approach, where we use transformers as feature extractors for a simple model."><meta name=author content="Aziz Baran"><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3299c596a7007118365635c056dd427dace22b7b8c1341fdef6fa6c31359ba10.css integrity="sha256-MpnFlqcAcRg2VjXAVt1CfaziK3uME0H972+mwxNZuhA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Transformers as Feature Extractors"><meta property="og:description" content="Transformers continue to be one of the most frequently used models for various NLP tasks since 2017. However, due to their high computational resource requirements and difficult maintenance, they may not be the most efficient choice out there all the time. This is especially true for simple sentiment classification tasks. In such circumstances, among the alternatives, there is the feature-based approach, where we use transformers as feature extractors for a simple model."><meta property="og:type" content="article"><meta property="og:url" content="https://azizbarank.github.io/post/transformers/"><meta property="og:image" content="https://azizbarank.github.io/%3Cimage%20path/url%3E"><meta property="article:section" content="post"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://azizbarank.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="Transformers as Feature Extractors"><meta name=twitter:description content="Transformers continue to be one of the most frequently used models for various NLP tasks since 2017. However, due to their high computational resource requirements and difficult maintenance, they may not be the most efficient choice out there all the time. This is especially true for simple sentiment classification tasks. In such circumstances, among the alternatives, there is the feature-based approach, where we use transformers as feature extractors for a simple model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://azizbarank.github.io/post/"},{"@type":"ListItem","position":2,"name":"Transformers as Feature Extractors","item":"https://azizbarank.github.io/post/transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformers as Feature Extractors","name":"Transformers as Feature Extractors","description":"Transformers continue to be one of the most frequently used models for various NLP tasks since 2017. However, due to their high computational resource requirements and difficult maintenance, they may not be the most efficient choice out there all the time. This is especially true for simple sentiment classification tasks. In such circumstances, among the alternatives, there is the feature-based approach, where we use transformers as feature extractors for a simple model.","keywords":[],"articleBody":"Transformers continue to be one of the most frequently used models for various NLP tasks since 2017. However, due to their high computational resource requirements and difficult maintenance, they may not be the most efficient choice out there all the time. This is especially true for simple sentiment classification tasks. In such circumstances, among the alternatives, there is the feature-based approach, where we use transformers as feature extractors for a simple model. What crucial in this approach is that since a transformer’s body weights are frozen, the hidden states need to be precomputed only once for them to be used as features for the model, meaning that the whole process does not require high computational resource compared to fine-tuning the whole model and provides an alternative to doing traditional machine learning and deep learning.\nIn this post, we will do a simple binary sentiment classification task using Rotten Tomatoes movie review dataset. We will obtain it through the Hugging Face Dataset library and make use of DistilRoBERTa to provide our simple Logistic Regression model with the features it needs to be trained with.\nQuick Intro: DistilRoBERTa DistilRoBERTa is a distilled version of the RoBERTa base model, which is an improved version of BERT due to longer training with more training data while using only Masked Language Modeling (MLM) objective. On average, DistilRoBERTa is twice as fast as RoBERTa because of having much less parameters of 82M (6 layers, 768 dimension and 12 heads).\n1. Environment Setup Our first step is to install the Hugging Face’s Dataset and Transformers libraries.\n!pip install datasets transformers Then, we need to load the other needed dependencies.\nimport torch import pandas as pd import numpy as np import matplotlib.pyplot as plt import sklearn 2. Exploring our Dataset We use the Rotten Tomatoes dataset for our purpose. It consists of movie reviews which are labeled as “1” and “2” that stand for “positive” and “negative”, respectively.\nWe use the Hugging Face Dataset library to load it.\nfrom datasets import load_dataset dataset = load_dataset(\"rotten_tomatoes\") Then, we can start looking at our dataset object:\ndataset # DatasetDict({ # train: Dataset({ # features: ['text', 'label'], # num_rows: 8530 # }) # validation: Dataset({ # features: ['text', 'label'], # num_rows: 1066 # }) # test: Dataset({ # features: ['text', 'label'], # num_rows: 1066 # }) #}) As can be seen, our dataset consists of three splits, each of them having two columns of “text” and “label”. Now, to explore our dataset further and with more convenience, let’s change its output format to a Pandas DataFrame so that we can inspect and visualize it more easily.\ndataset.set_format(type=\"pandas\") df = dataset[\"train\"][:] df.head() # text label #0\tthe rock is destined to be the 21st century's ...\t1 #1\tthe gorgeously elaborate continuation of \" the...\t1 #2\teffective but too-tepid biopic\t1 #3\tif you sometimes like to go to the movies to h...\t1 #4\temerges as something rare , an issue movie tha...\t1 Then, for future use, let’s also add a column that corresponds to “positive” and “negative” for the integers of “1” and “2”, respectively.\ndef label_str(row): return dataset[\"train\"].features[\"label\"].str(row) df[\"label_name\"] = df[\"label\"].apply(label_str) df.head() # text label label_name #0\tthe rock is destined to be the 21st century's ...\t1\tpos #1\tthe gorgeously elaborate continuation of \" the...\t1\tpos #2\teffective but too-tepid biopic\t1\tpos #3\tif you sometimes like to go to the movies to h...\t1\tpos #4\temerges as something rare , an issue movie tha...\t1\tpos Finally, as part of exploring the dataset, we can look at its class distribution to see whether it is balanced or not. To do this, we can use Pandas and Matplotlib.\ndf[\"label_name\"].value_counts(ascending=True).plot.barh() plt.title(\"Frequency of Classes\") plt.show() # neg= 5000 # pos = 5000 As can be seen, our dataset is pretty balanced as it contains the same amount of “positive” and “negative” labels. Therefore, we don’t have to apply any methods used for class imbalance.\n3. Preprocessing Now that we have enough insight on our dataset, we can start preprocessing it. In this context, preprocessing involves tokenization and then encoding our raw text strings so that we can feed them through our classification model. To achieve this, as said at the beginning of this post, we are going to use DistilRoBERTa.\nFirst, we need to load the tokenizer of it through AutoTokenizer class of Hugging Face Transformers library.\nfrom transformers import AutoTokenizer model_ckpt = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_ckpt) Then, since we are dealing with a dataset instead of a single string, we should define a function to apply our model for a whole tokenization.\ndef tokenize(batch): return tokenizer(batch[\"text\"], padding=True, truncation=True) Now, we will use the map() method to apply this function to our dataset as a single batch.\ndataset_encoded = dataset.map(tokenize, batched=True, batch_size=None) print(dataset_encoded[\"train\"].column_names) #['text', 'label', 'input_ids', 'attention_mask'] 4. Obtaining The Hidden States Now that we have our token encodings, we can proceed to the next step, which is obtaining the last hidden state to fit our classification model with. Basically, to do this, we need to convert our token encodings to token embeddings and then feed them through encoder stack to get our hidden states, all of which can be done through the AutoModel class of Hugging Face Transformers. Since we used the DistilRoBERTa’s tokenizer, it would be the best to use the same model here as well.\nLet’s load the model first.\nfrom transformers import AutoModel model_ckpt = \"distilbert-base-uncased\" device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model = AutoModel.from_pretrained(model_ckpt).to(device) Now, there is something to keep in mind. To get embeddings from our encoded tokens, our model needs PyTorch tensors. Only then can we apply our function to get the last hidden states. Therefore, from now on we have two things to do:\nDefine a function to be applied to the whole dataset via map() method. Before using this function immediately, convert the encoded tokens we obtained before to PyTorch tensors. Let’s do the first step:\ndef extract_hidden_states(batch): inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names} with torch.no_grad(): last_hidden_state = model(**inputs).last_hidden_state return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()} Then the second step:\ndataset_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]) Finally, we can use the function:\ndataset_hidden = dataset_encoded.map(extract_hidden_states, batched=True) As a result, a new column that consists of hidden state vectors is added to our dataset:\ndataset_hidden[\"train\"].column_names #['text', 'label', 'input_ids', 'attention_mask', 'hidden_state'] 5. Training \u0026 Results Since we finally have our input features, by using the corresponding labels, we can create a feature matrix. To do this, famous Python library scikit-learn can be used.\nX_train = np.array(dataset_hidden[\"train\"][\"hidden_state\"]) X_valid = np.array(dataset_hidden[\"validation\"][\"hidden_state\"]) y_train = np.array(dataset_hidden[\"train\"][\"label\"]) y_valid = np.array(dataset_hidden[\"validation\"][\"label\"]) Now, let’s fit our Logistic Regression model and see the results in terms of accuracy.\nfrom sklearn.linear_model import LogisticRegression clf = LogisticRegression(max_iter=3000) clf.fit(X_train, y_train) clf.score(X_valid, y_valid) #0.8292682926829268 This seems like a really impressive result. However, to be sure how “great” our accuracy is, we can use scikit-learn’s DummyClassifier to get a baseline to compare our result with.\nfrom sklearn.dummy import DummyClassifier dummy_clf = DummyClassifier(strategy=\"most_frequent\") dummy_clf.fit(X_train, y_train) dummy_clf.score(X_valid, y_valid) #0.5 Indeed, our model did a very good job!\nConclusion We successfully used DistilRoBERTa’s embeddings and a Logistic Regression model to do a binary sentiment classification on movie reviews from Rotten Tomaotes. In the end, we managed to get a 83% accuracy without spending high computational resource.\nIt should not be forgotten that this method is applicable to other tasks and that the other models can be used instead of DistilRoBERTa as well.\n","wordCount":"1239","inLanguage":"en","image":"https://azizbarank.github.io/%3Cimage%20path/url%3E","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Aziz Baran"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://azizbarank.github.io/post/transformers/"},"publisher":{"@type":"Organization","name":"Aziz Baran","logo":{"@type":"ImageObject","url":"https://azizbarank.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://azizbarank.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://azizbarank.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://azizbarank.github.io/about/ title=About><span>About</span></a></li><li><a href=https://azizbarank.github.io/post/ title=Post><span>Post</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://azizbarank.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://azizbarank.github.io/post/>Posts</a></div><h1 class=post-title>Transformers as Feature Extractors</h1><div class=post-meta>6 min&nbsp;·&nbsp;1239 words&nbsp;·&nbsp;Aziz Baran&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/post/transformers.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#quick-intro-distilroberta>Quick Intro: DistilRoBERTa</a></li><li><a href=#1-environment-setup>1. Environment Setup</a></li><li><a href=#2-exploring-our-dataset>2. Exploring our Dataset</a></li><li><a href=#3-preprocessing>3. Preprocessing</a></li><li><a href=#4-obtaining-the-hidden-states>4. Obtaining The Hidden States</a></li><li><a href=#5-training--results>5. Training & Results</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><p>Transformers continue to be one of the most frequently used models for various NLP tasks since 2017. However, due to their high computational resource requirements and difficult maintenance, they may not be the most efficient choice out there all the time. This is especially true for simple sentiment classification tasks. In such circumstances, among the alternatives, there is the feature-based approach, where we use transformers as feature extractors for a simple model. What crucial in this approach is that since a transformer’s body weights are frozen, the hidden states need to be precomputed only once for them to be used as features for the model, meaning that the whole process does not require high computational resource compared to fine-tuning the whole model and provides an alternative to doing traditional machine learning and deep learning.</p><p>In this post, we will do a simple binary sentiment classification task using Rotten Tomatoes movie review dataset. We will obtain it through the Hugging Face Dataset library and make use of DistilRoBERTa to provide our simple Logistic Regression model with the features it needs to be trained with.</p><h2 id=quick-intro-distilroberta>Quick Intro: DistilRoBERTa<a hidden class=anchor aria-hidden=true href=#quick-intro-distilroberta>#</a></h2><p><a href=https://huggingface.co/distilroberta-base>DistilRoBERTa</a> is a distilled version of the RoBERTa base model, which is an improved version of BERT due to longer training with more training data while using only Masked Language Modeling (MLM) objective. On average, DistilRoBERTa is twice as fast as RoBERTa because of having much less parameters of 82M (6 layers, 768 dimension and 12 heads).</p><h2 id=1-environment-setup>1. Environment Setup<a hidden class=anchor aria-hidden=true href=#1-environment-setup>#</a></h2><p>Our first step is to install the Hugging Face’s Dataset and Transformers libraries.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=n>datasets</span> <span class=n>transformers</span>
</span></span></code></pre></div><p>Then, we need to load the other needed dependencies.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sklearn</span>
</span></span></code></pre></div><h2 id=2-exploring-our-dataset>2. Exploring our Dataset<a hidden class=anchor aria-hidden=true href=#2-exploring-our-dataset>#</a></h2><p>We use the Rotten Tomatoes dataset for our purpose. It consists of movie reviews which are labeled as “1” and “2” that stand for “positive” and “negative”, respectively.</p><p>We use the Hugging Face Dataset library to load it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;rotten_tomatoes&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Then, we can start looking at our dataset object:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dataset</span>
</span></span><span class=line><span class=cl><span class=c1># DatasetDict({</span>
</span></span><span class=line><span class=cl><span class=c1>#   train: Dataset({</span>
</span></span><span class=line><span class=cl><span class=c1>#        features: [&#39;text&#39;, &#39;label&#39;],</span>
</span></span><span class=line><span class=cl><span class=c1>#        num_rows: 8530</span>
</span></span><span class=line><span class=cl><span class=c1>#    })</span>
</span></span><span class=line><span class=cl><span class=c1>#    validation: Dataset({</span>
</span></span><span class=line><span class=cl><span class=c1>#        features: [&#39;text&#39;, &#39;label&#39;],</span>
</span></span><span class=line><span class=cl><span class=c1>#        num_rows: 1066</span>
</span></span><span class=line><span class=cl><span class=c1>#    })</span>
</span></span><span class=line><span class=cl><span class=c1>#    test: Dataset({</span>
</span></span><span class=line><span class=cl><span class=c1>#        features: [&#39;text&#39;, &#39;label&#39;],</span>
</span></span><span class=line><span class=cl><span class=c1>#        num_rows: 1066</span>
</span></span><span class=line><span class=cl><span class=c1>#    })</span>
</span></span><span class=line><span class=cl><span class=c1>#})</span>
</span></span></code></pre></div><p>As can be seen, our dataset consists of three splits, each of them having two columns of “text” and “label”.
Now, to explore our dataset further and with more convenience, let’s change its output format to a Pandas DataFrame so that we can inspect and visualize it more easily.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dataset</span><span class=o>.</span><span class=n>set_format</span><span class=p>(</span><span class=nb>type</span><span class=o>=</span><span class=s2>&#34;pandas&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>dataset</span><span class=p>[</span><span class=s2>&#34;train&#34;</span><span class=p>][:]</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1>#                                                    text   label</span>
</span></span><span class=line><span class=cl><span class=c1>#0	the rock is destined to be the 21st century&#39;s ...	1</span>
</span></span><span class=line><span class=cl><span class=c1>#1	the gorgeously elaborate continuation of &#34; the...	1</span>
</span></span><span class=line><span class=cl><span class=c1>#2	effective but too-tepid biopic	1</span>
</span></span><span class=line><span class=cl><span class=c1>#3	if you sometimes like to go to the movies to h...	1</span>
</span></span><span class=line><span class=cl><span class=c1>#4	emerges as something rare , an issue movie tha...	1</span>
</span></span></code></pre></div><p>Then, for future use, let’s also add a column that corresponds to “positive” and “negative” for the integers of “1” and “2”, respectively.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>label_str</span><span class=p>(</span><span class=n>row</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>dataset</span><span class=p>[</span><span class=s2>&#34;train&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>features</span><span class=p>[</span><span class=s2>&#34;label&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>str</span><span class=p>(</span><span class=n>row</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s2>&#34;label_name&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s2>&#34;label&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>label_str</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#                                                    text   label label_name</span>
</span></span><span class=line><span class=cl><span class=c1>#0	the rock is destined to be the 21st century&#39;s ...	1	pos</span>
</span></span><span class=line><span class=cl><span class=c1>#1	the gorgeously elaborate continuation of &#34; the...	1	pos</span>
</span></span><span class=line><span class=cl><span class=c1>#2	effective but too-tepid biopic	1	pos</span>
</span></span><span class=line><span class=cl><span class=c1>#3	if you sometimes like to go to the movies to h...	1	pos</span>
</span></span><span class=line><span class=cl><span class=c1>#4	emerges as something rare , an issue movie tha...	1	pos</span>
</span></span></code></pre></div><p>Finally, as part of exploring the dataset, we can look at its class distribution to see whether it is balanced or not. To do this, we can use Pandas and Matplotlib.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s2>&#34;label_name&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>value_counts</span><span class=p>(</span><span class=n>ascending</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=o>.</span><span class=n>barh</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Frequency of Classes&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># neg= 5000</span>
</span></span><span class=line><span class=cl><span class=c1># pos = 5000</span>
</span></span></code></pre></div><p>As can be seen, our dataset is pretty balanced as it contains the same amount of “positive” and “negative” labels. Therefore, we don’t have to apply any methods used for class imbalance.</p><h2 id=3-preprocessing>3. Preprocessing<a hidden class=anchor aria-hidden=true href=#3-preprocessing>#</a></h2><p>Now that we have enough insight on our dataset, we can start preprocessing it. In this context, preprocessing involves tokenization and then encoding our raw text strings so that we can feed them through our classification model. To achieve this, as said at the beginning of this post, we are going to use DistilRoBERTa.</p><p>First, we need to load the tokenizer of it through AutoTokenizer class of Hugging Face Transformers library.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_ckpt</span> <span class=o>=</span> <span class=s2>&#34;distilbert-base-uncased&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_ckpt</span><span class=p>)</span>
</span></span></code></pre></div><p>Then, since we are dealing with a dataset instead of a single string, we should define a function to apply our model for a whole tokenization.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>tokenize</span><span class=p>(</span><span class=n>batch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=s2>&#34;text&#34;</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>Now, we will use the map() method to apply this function to our dataset as a single batch.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dataset_encoded</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>tokenize</span><span class=p>,</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>dataset_encoded</span><span class=p>[</span><span class=s2>&#34;train&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>column_names</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#[&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;]</span>
</span></span></code></pre></div><h2 id=4-obtaining-the-hidden-states>4. Obtaining The Hidden States<a hidden class=anchor aria-hidden=true href=#4-obtaining-the-hidden-states>#</a></h2><p>Now that we have our token encodings, we can proceed to the next step, which is obtaining the last hidden state to fit our classification model with. Basically, to do this, we need to convert our token encodings to token embeddings and then feed them through encoder stack to get our hidden states, all of which can be done through the AutoModel class of Hugging Face Transformers. Since we used the DistilRoBERTa’s tokenizer, it would be the best to use the same model here as well.</p><p>Let’s load the model first.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_ckpt</span> <span class=o>=</span> <span class=s2>&#34;distilbert-base-uncased&#34;</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_ckpt</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span></code></pre></div><p>Now, there is something to keep in mind. To get embeddings from our encoded tokens, our model needs PyTorch tensors. Only then can we apply our function to get the last hidden states. Therefore, from now on we have two things to do:</p><ul><li>Define a function to be applied to the whole dataset via map() method.</li><li>Before using this function immediately, convert the encoded tokens we obtained before to PyTorch tensors.</li></ul><p>Let’s do the first step:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>extract_hidden_states</span><span class=p>(</span><span class=n>batch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>inputs</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span><span class=n>v</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span><span class=n>v</span> <span class=ow>in</span> <span class=n>batch</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>model_input_names</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>last_hidden_state</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span><span class=o>.</span><span class=n>last_hidden_state</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;hidden_state&#34;</span><span class=p>:</span> <span class=n>last_hidden_state</span><span class=p>[:,</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>()}</span>
</span></span></code></pre></div><p>Then the second step:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dataset_encoded</span><span class=o>.</span><span class=n>set_format</span><span class=p>(</span><span class=s2>&#34;torch&#34;</span><span class=p>,</span> <span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;input_ids&#34;</span><span class=p>,</span> <span class=s2>&#34;attention_mask&#34;</span><span class=p>,</span> <span class=s2>&#34;label&#34;</span><span class=p>])</span>
</span></span></code></pre></div><p>Finally, we can use the function:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dataset_hidden</span> <span class=o>=</span> <span class=n>dataset_encoded</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>extract_hidden_states</span><span class=p>,</span> <span class=n>batched</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>As a result, a new column that consists of hidden state vectors is added to our dataset:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dataset_hidden</span><span class=p>[</span><span class=s2>&#34;train&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>column_names</span>
</span></span><span class=line><span class=cl><span class=c1>#[&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;hidden_state&#39;]</span>
</span></span></code></pre></div><h2 id=5-training--results>5. Training & Results<a hidden class=anchor aria-hidden=true href=#5-training--results>#</a></h2><p>Since we finally have our input features, by using the corresponding labels, we can create a feature matrix. To do this, famous Python library scikit-learn can be used.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X_train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>dataset_hidden</span><span class=p>[</span><span class=s2>&#34;train&#34;</span><span class=p>][</span><span class=s2>&#34;hidden_state&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>X_valid</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>dataset_hidden</span><span class=p>[</span><span class=s2>&#34;validation&#34;</span><span class=p>][</span><span class=s2>&#34;hidden_state&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>y_train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>dataset_hidden</span><span class=p>[</span><span class=s2>&#34;train&#34;</span><span class=p>][</span><span class=s2>&#34;label&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>y_valid</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>dataset_hidden</span><span class=p>[</span><span class=s2>&#34;validation&#34;</span><span class=p>][</span><span class=s2>&#34;label&#34;</span><span class=p>])</span>
</span></span></code></pre></div><p>Now, let’s fit our Logistic Regression model and see the results in terms of accuracy.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>clf</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>(</span><span class=n>max_iter</span><span class=o>=</span><span class=mi>3000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>clf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>clf</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_valid</span><span class=p>,</span> <span class=n>y_valid</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#0.8292682926829268</span>
</span></span></code></pre></div><p>This seems like a really impressive result. However, to be sure how “great” our accuracy is, we can use scikit-learn’s DummyClassifier to get a baseline to compare our result with.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.dummy</span> <span class=kn>import</span> <span class=n>DummyClassifier</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dummy_clf</span> <span class=o>=</span> <span class=n>DummyClassifier</span><span class=p>(</span><span class=n>strategy</span><span class=o>=</span><span class=s2>&#34;most_frequent&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dummy_clf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dummy_clf</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>X_valid</span><span class=p>,</span> <span class=n>y_valid</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#0.5</span>
</span></span></code></pre></div><p>Indeed, our model did a very good job!</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>We successfully used DistilRoBERTa’s embeddings and a Logistic Regression model to do a binary sentiment classification on movie reviews from Rotten Tomaotes. In the end, we managed to get a 83% accuracy without spending high computational resource.</p><p>It should not be forgotten that this method is applicable to other tasks and that the other models can be used instead of DistilRoBERTa as well.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://azizbarank.github.io/post/deployment/><span class=title>« Prev</span><br><span>Simple Guide to Building and Deploying Transformer Models with Hugging Face</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://azizbarank.github.io/>Aziz Baran</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>